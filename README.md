# Supervised Sentiment Analysis: Word2Vec as an input into Logistic Regression on IMDB Dataset

## Word2Vec and Word Embeddings

Word embeddings are a type of representation used in natural language processing (NLP) that allows words to be represented as dense vectors in a high-dimensional space. These vectors capture semantic and syntactic information about the words they represent, allowing NLP models to better understand the meaning of language.

Word2vec is a popular algorithm for training word embeddings. It works by analyzing the co-occurrence patterns of words in a large corpus of text and learning to predict which words are likely to appear near each other.
